{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Open a file: file\n",
    "sent_table = pd.read_csv('/home/dineshp/Poetix18/data/rhet/corpus.txt',usecols=['text'])\n",
    "#print(sent_table)\n",
    "\n",
    "\n",
    "for i in sent_table.index.values:\n",
    "    text = sent_table.iloc[i]['text'];\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace('''O ''', '''Oh ''')\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace('''`''', ''' \\'''')\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace(''' ` ''', ''' \\'''')\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace(''' `''', ''' \\'''')\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace(''' \\'''', '''\\'''')\n",
    "    #sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].replace('''\\'d''', '''ed''')\n",
    "    sent_table.iloc[i]['text'] = re.sub(r'''[^\\w\\d'\\-\\s]+''','',sent_table.iloc[i]['text']);\n",
    "    sent_table.iloc[i]['text'] = sent_table.iloc[i]['text'].strip();\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def uniq(input):\n",
    "  output = []\n",
    "  for x in input:\n",
    "    if x not in output:\n",
    "      output.append(x)\n",
    "  return output\n",
    "\n",
    "def recursive_len(item):\n",
    "    if type(item) == list:\n",
    "        return sum(recursive_len(subitem) for subitem in item)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "num_figs=10\n",
    "max_fig_length=18\n",
    "base_path = '/home/dineshp/Poetix18/data/rhet/repitition/'\n",
    "base_path_output = '/home/dineshp/Poetix18/data/rhet/output/'\n",
    "corpus = 'sonnets'\n",
    "\n",
    "fig_names = [\"Anadiplosis\", \"Anaphora\", \"Antimetabole\", \"Conduplicatio\", \"Epanalepsis\", \"Epistrophe\", \"Epizeuxis\",\n",
    "             \"Ploce\", \"Polysyndeton\", \"Symploce\"]\n",
    "\n",
    "fig_descs = \\\n",
    "    [\n",
    "        \"Repetition of the ending word or phrase from the previous clause at the beginning of the next.\",\n",
    "        \"Repetition of a word or phrase at the beginning of successive phrases or clauses.\",\n",
    "        \"Repetition of words in reverse grammatical order.\",\n",
    "        \"The repetition of a word or phrase.\",\n",
    "        \"Repetition at the end of a clause of the word or phrase that began it.\",\n",
    "        \"Repetition of the same word or phrase at the end of successive clauses.\",\n",
    "        \"Repetition of a word or phrase with no others between.\",\n",
    "        \"The repetition of word in a short span of text for rhetorical emphasis.\",\n",
    "        \"\\\"Excessive\\\" repetition of conjunctions between clauses.\",\n",
    "        \"Repetition of a word or phrase at the beginning, and of another at the end, of successive clauses.\"\n",
    "    ]\n",
    "\n",
    "rep_figs = [\"sonnet_anadiplosis.csv\", \"sonnet_anaphora.csv\", \"sonnet_antimetabole.csv\", \"sonnet_conduplicatio.csv\", \n",
    "            \"sonnet_epanalepsis.csv\", \"sonnet_epistrophe.csv\", \"sonnet_epizeuxis.csv\", \"sonnet_ploce.csv\", \n",
    "            \"sonnet_polysyndeton.csv\", \"sonnet_symploce.csv\"]\n",
    "\n",
    "\n",
    "\n",
    "for nfig in range(0,num_figs):\n",
    "    \n",
    "    cur_fig_name = fig_names[nfig]\n",
    "    cur_fig_desc = fig_descs[nfig]\n",
    "    cur_fig_rhet_file = rep_figs[nfig]\n",
    "\n",
    "    rhet_table = pd.read_csv(base_path+cur_fig_rhet_file)\n",
    "    timestamp = int(time.mktime(datetime.now().timetuple()))\n",
    "    out_name = base_path_output + corpus + \"_\" + cur_fig_name + \"_\" + str(timestamp) + \".txt\"\n",
    "    f = open(out_name,\"w+\")\n",
    "    \n",
    "    f.write(\"FIGURE: %s\\n\" % cur_fig_name)\n",
    "    f.write(\"DESCRIPTIOM: %s\\n\" % cur_fig_desc)\n",
    "    f.write(\"CORPUS: %s\\n\\n\" % corpus)\n",
    "    fig_ids = rhet_table.figure_id.unique()\n",
    "    #print(fig_ids)\n",
    "    for id in fig_ids:\n",
    "       figure = rhet_table.loc[rhet_table['figure_id'] == id]\n",
    "       rep_word = []\n",
    "       sentence_ids = figure['sentence_id'].values\n",
    "       sentence_ids = uniq(sentence_ids)\n",
    "       #print(sentence_ids)\n",
    "       fig_words = uniq([x.lower().replace('''\\'d''', '''ed''') for x in list(figure['word'].values)])\n",
    "       fig_words_dict = dict([[fig_words[y-1], y] for y in range(1, len(fig_words)+1)])\n",
    "       #print(fig_words_dict)\n",
    "    \n",
    "       orig_text = [ nltk.word_tokenize(sent_table.iloc[i]['text']) for i in sentence_ids]\n",
    "       pattern = '^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$'\n",
    "       roman = []\n",
    "       for j in range(0,len(orig_text)):\n",
    "           for k in range(0,len(orig_text[j])):\n",
    "               if(re.search(pattern, orig_text[j][k])):\n",
    "                   roman.append([j,k])\n",
    "       \n",
    "       for l in range(0,len(roman)):\n",
    "           del orig_text[roman[l][0]][roman[l][1]]\n",
    "    \n",
    "       orig_text = [[orig_text[i][j].lower() for j in range(0,len(orig_text[i]))] for i in range(0,len(orig_text))]\n",
    "       #if(len(sentence_ids) == 1):\n",
    "       tagged = [nltk.pos_tag(orig_text[i]) for i in  range(0,len(orig_text))]\n",
    "       #print(orig_text)\n",
    "       pos_templates = [[tagged[i][j][1] for j in range(0,len(tagged[i]))] for i in range(0,len(tagged))]\n",
    "       #print(pos_templates)\n",
    "       \n",
    "       assert(np.shape(orig_text) == np.shape(pos_templates))\n",
    "       fig_words_count_dict = dict([[fig_words[y], 0] for y in range(0, len(fig_words))])\n",
    "    \n",
    "       rep_templates = [['0' for j in range(0,len(orig_text[i]))] for i in range(0,len(orig_text))]\n",
    "       for j in range(0,len(orig_text)):\n",
    "           for k in range(0,len(orig_text[j])):\n",
    "               wordc = orig_text[j][k]\n",
    "               for word_rep in list(fig_words_dict.keys()):\n",
    "                   if(word_rep in wordc):\n",
    "                       fig_words_count_dict[word_rep] = fig_words_count_dict[word_rep]+1\n",
    "                       rep_templates[j][k] = str(fig_words_dict[word_rep]) + str(fig_words_count_dict[word_rep])\n",
    "       \n",
    "       f.write(\"Orig. Text:\\n\")\n",
    "       f.write(\", \".join([' '.join(orig_text[i]) for i in  range(0,len(orig_text))]))\n",
    "       f.write(\"\\n\")\n",
    "       f.write(\"Rep. Words:\\n\")\n",
    "       f.writelines(', '.join(fig_words_dict))\n",
    "       f.write(\"\\n\")\n",
    "       f.write(\"POS Template:\\n\")\n",
    "       f.writelines(str(pos_templates))\n",
    "       f.write(\"\\n\")\n",
    "       f.write(\"Rep Template:\\n\")\n",
    "       f.writelines(str(rep_templates))\n",
    "       f.write(\"\\n------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "       #print(rep_templates)     \n",
    "  \n",
    "   #for index, row in figure.iterrows():\n",
    "    f.close()\n",
    "   #source = sentences\n",
    "   # while(!re.search(exp, source)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "thou mayst call thine when thou from youth convertest\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dineshp/Poetix18/gpt2/src/score.py:55: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/345M/model.ckpt\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-90359ef3a1ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m            \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m            \u001b[0mnew_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_line_gpt_cc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcctemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep_templates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_templates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbanned_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbanned_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m            \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m            \u001b[0msent_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poetix18/py_files/Limericks.py\u001b[0m in \u001b[0;36mgen_line_gpt_cc\u001b[0;34m(self, cctemplate, w, encodes, default_template, rhyme_word, rhyme_set, banned_set, search_space)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# Logits is the output of GPT model, encoder is used to decode the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mPOS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mCC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcctemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poetix18/gpt2/src/score.py\u001b[0m in \u001b[0;36mscore_model\u001b[0;34m(model_name, seed, nsamples, length, temperature, top_k, context_token)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         out = sess.run(logits, feed_dict={\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         })\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (0,) for Tensor 'context_6:0', which has shape '(0, ?)'"
     ],
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (0,) for Tensor 'context_6:0', which has shape '(0, ?)'",
     "output_type": "error"
    }
   ],
   "source": [
    "def uniq(input):\n",
    "  output = []\n",
    "  for x in input:\n",
    "    if x not in output:\n",
    "      output.append(x)\n",
    "  return output\n",
    "\n",
    "def recursive_len(item):\n",
    "    if type(item) == list:\n",
    "        return sum(recursive_len(subitem) for subitem in item)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "num_figs=10\n",
    "max_fig_length=18\n",
    "base_path = '/home/dineshp/Poetix18/data/rhet/repitition/'\n",
    "base_path_output = '/home/dineshp/Poetix18/data/rhet/output_sample_gen/'\n",
    "corpus = 'sonnets'\n",
    "\n",
    "fig_names = [\"Anadiplosis\", \"Anaphora\", \"Antimetabole\", \"Conduplicatio\", \"Epanalepsis\", \"Epistrophe\", \"Epizeuxis\",\n",
    "             \"Ploce\", \"Polysyndeton\", \"Symploce\"]\n",
    "\n",
    "fig_descs = \\\n",
    "    [\n",
    "        \"Repetition of the ending word or phrase from the previous clause at the beginning of the next.\",\n",
    "        \"Repetition of a word or phrase at the beginning of successive phrases or clauses.\",\n",
    "        \"Repetition of words in reverse grammatical order.\",\n",
    "        \"The repetition of a word or phrase.\",\n",
    "        \"Repetition at the end of a clause of the word or phrase that began it.\",\n",
    "        \"Repetition of the same word or phrase at the end of successive clauses.\",\n",
    "        \"Repetition of a word or phrase with no others between.\",\n",
    "        \"The repetition of word in a short span of text for rhetorical emphasis.\",\n",
    "        \"\\\"Excessive\\\" repetition of conjunctions between clauses.\",\n",
    "        \"Repetition of a word or phrase at the beginning, and of another at the end, of successive clauses.\"\n",
    "    ]\n",
    "\n",
    "rep_figs = [\"sonnet_anadiplosis.csv\", \"sonnet_anaphora.csv\", \"sonnet_antimetabole.csv\", \"sonnet_conduplicatio.csv\", \n",
    "            \"sonnet_epanalepsis.csv\", \"sonnet_epistrophe.csv\", \"sonnet_epizeuxis.csv\", \"sonnet_ploce.csv\", \n",
    "            \"sonnet_polysyndeton.csv\", \"sonnet_symploce.csv\"]\n",
    "\n",
    "from py_files.Limericks import Limerick_Generate\n",
    "lg = Limerick_Generate(model_dir='/gpt2/models/345M',model_name='345M')\n",
    "n=50\n",
    "banned_set = ['foo']\n",
    "for nfig in range(0,num_figs):\n",
    "    \n",
    "    cur_fig_name = fig_names[nfig]\n",
    "    cur_fig_desc = fig_descs[nfig]\n",
    "    cur_fig_rhet_file = rep_figs[nfig]\n",
    "\n",
    "    rhet_table = pd.read_csv(base_path+cur_fig_rhet_file)\n",
    "    timestamp = int(time.mktime(datetime.now().timetuple()))\n",
    "    out_name = base_path_output + corpus + \"_\" + cur_fig_name + \"_\" + str(timestamp) + \".txt\"\n",
    "    f = open(out_name,\"w+\")\n",
    "    \n",
    "    f.write(\"FIGURE: %s\\n\" % cur_fig_name)\n",
    "    f.write(\"DESCRIPTIOM: %s\\n\" % cur_fig_desc)\n",
    "    f.write(\"CORPUS: %s\\n\\n\" % corpus)\n",
    "    fig_ids = rhet_table.figure_id.unique()\n",
    "    #print(fig_ids)\n",
    "    for id in fig_ids:\n",
    "       figure = rhet_table.loc[rhet_table['figure_id'] == id]\n",
    "       rep_word = []\n",
    "       sentence_ids = figure['sentence_id'].values\n",
    "       sentence_ids = uniq(sentence_ids)\n",
    "       #print(sentence_ids)\n",
    "       fig_words = uniq([x.lower().replace('''\\'d''', '''ed''') for x in list(figure['word'].values)])\n",
    "       fig_words_dict = dict([[fig_words[y-1], y] for y in range(1, len(fig_words)+1)])\n",
    "       #print(fig_words_dict)\n",
    "    \n",
    "       orig_text = [ nltk.word_tokenize(sent_table.iloc[i]['text']) for i in sentence_ids]\n",
    "       pattern = '^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$'\n",
    "       roman = []\n",
    "       for j in range(0,len(orig_text)):\n",
    "           for k in range(0,len(orig_text[j])):\n",
    "               if(re.search(pattern, orig_text[j][k])):\n",
    "                   roman.append([j,k])\n",
    "       \n",
    "       for l in range(0,len(roman)):\n",
    "           del orig_text[roman[l][0]][roman[l][1]]\n",
    "    \n",
    "       orig_text = [[orig_text[i][j].lower() for j in range(0,len(orig_text[i]))] for i in range(0,len(orig_text))]\n",
    "       if(len(sentence_ids) == 1):\n",
    "           tagged = [nltk.pos_tag(orig_text[i]) for i in  range(0,len(orig_text))]\n",
    "           #print(orig_text)\n",
    "           pos_templates = [[tagged[i][j][1] for j in range(0,len(tagged[i]))] for i in range(0,len(tagged))]\n",
    "           #print(pos_templates)\n",
    "           \n",
    "           assert(np.shape(orig_text) == np.shape(pos_templates))\n",
    "           fig_words_count_dict = dict([[fig_words[y], 0] for y in range(0, len(fig_words))])\n",
    "        \n",
    "           rep_templates = [['0' for j in range(0,len(orig_text[i]))] for i in range(0,len(orig_text))]\n",
    "           for j in range(0,len(orig_text)):\n",
    "               for k in range(0,len(orig_text[j])):\n",
    "                   wordc = orig_text[j][k]\n",
    "                   for word_rep in list(fig_words_dict.keys()):\n",
    "                       if(word_rep in wordc):\n",
    "                           fig_words_count_dict[word_rep] = fig_words_count_dict[word_rep]+1\n",
    "                           rep_templates[j][k] = str(fig_words_dict[word_rep]) + str(fig_words_count_dict[word_rep])\n",
    "           \n",
    "           f.write(\"Orig. Text:\\n\")\n",
    "           orig_line = \", \".join([' '.join(orig_text[i]) for i in  range(0,len(orig_text))])\n",
    "           print(orig_line)\n",
    "           f.write(orig_line)\n",
    "           f.write(\"\\n\")\n",
    "           f.write(\"Rep. Words:\\n\")\n",
    "           f.writelines(', '.join(fig_words_dict))\n",
    "           f.write(\"\\n\")\n",
    "           f.write(\"POS Template:\\n\")\n",
    "           f.writelines(str(pos_templates))\n",
    "           f.write(\"\\n\")\n",
    "           f.write(\"Rep Template:\\n\")\n",
    "           f.writelines(str(rep_templates))\n",
    "           f.write(\"\\n\")\n",
    "           f.write(\"Sample Output:\\n\")\n",
    "           \n",
    "           prompt = lg.enc.encode(orig_line)\n",
    "           new_sentences = lg.gen_line_gpt_cc(cctemplate=rep_templates[0], w=None, encodes=prompt, default_template=pos_templates[0], banned_set=banned_set,search_space=n)\n",
    "           sents = [new_sentences[j][0] for j in range(n)]\n",
    "           sent_done = [' '.join(sents[j]) for j in range(n)]\n",
    "           f.write(\" \\n\".join(sent_done))\n",
    "           f.write(\"\\n------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "       #print(rep_templates)     \n",
    "  \n",
    "   #for index, row in figure.iterrows():\n",
    "    f.close()\n",
    "   #source = sentences\n",
    "   # while(!re.search(exp, source)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}