{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from py_files.Limericks import Limerick_Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = Limerick_Generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('template.txt','r')\n",
    "template = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-02ad26960af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m lg.gen_line_with_template(template, (['PRP', 'VBD', 'CD', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN'], \n\u001b[0;32m----> 2\u001b[0;31m                                      ['he', 'was', 'one', 'funny', 'brit', 'with', 'a', 'nice', 'head', 'of', 'hair', 'on', 'his', 'pate'], 4))\n\u001b[0m",
      "\u001b[0;32m~/Desktop/poetix/py_files/Limericks.py\u001b[0m in \u001b[0;36mgen_line_with_template\u001b[0;34m(self, prompt, template)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mline\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mBGT\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msatisfies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[0mPOS\u001b[0m \u001b[0mrestrictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \"\"\"\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"current\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_line\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/poetix/gpt2/src/score.py\u001b[0m in \u001b[0;36mscore_model\u001b[0;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, input)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcontext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/poetix/gpt2/src/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mbpe_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbpe_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbpe_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/regex.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     more than one group. Empty matches are included in the result.\"\"\"\n\u001b[1;32m    332\u001b[0m     return _compile(pattern, flags, kwargs).findall(string, pos, endpos,\n\u001b[0;32m--> 333\u001b[0;31m       overlapped, concurrent)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m def finditer(pattern, string, flags=0, pos=None, endpos=None, overlapped=False,\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "lg.gen_line_with_template(template, (['PRP', 'VBD', 'CD', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN'], \n",
    "                                     ['he', 'was', 'one', 'funny', 'brit', 'with', 'a', 'nice', 'head', 'of', 'hair', 'on', 'his', 'pate'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from gpt2/models/117M/model.ckpt\n",
      "[('he was one funny brit with a nice head of hair on his pate', -176.0026502609253)]\n"
     ]
    }
   ],
   "source": [
    "lg.gen_line_with_template(template, (['PRP', 'VBD', 'CD', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN'], \n",
    "                                     ['he', 'was', 'one', 'funny', 'brit', 'with', 'a', 'nice', 'head', 'of', 'hair', 'on', 'his', 'pate'], 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
