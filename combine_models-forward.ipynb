{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import nltk\n",
    "import time\n",
    "import queue as Q\n",
    "from operator import itemgetter\n",
    "from model import Model\n",
    "import itertools\n",
    "from numpy.random import choice\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "import sys\n",
    "import pickle\n",
    "import getopt\n",
    "from Traversal import *\n",
    "from extra_func import *\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN NEXT CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('./storyline_for_reference/glove.6B.300d.word2vec.txt',binary=False)\n",
    "  # system arguments\n",
    "#topics = [sys.argv[1]]\n",
    "try:\n",
    "    seed = int(sys.argv[2])\n",
    "except:\n",
    "    seed = 1\n",
    "np.random.seed(seed) # seed for reproducibility\n",
    "\n",
    "num_per_topic = 1 \n",
    "#topic=topics[0]\n",
    "text_list = [(\"data\\\\frost\\\\input.txt\",\"frost_model_forward\"),(\"data\\whitman\\input.txt\",\"whitman_model\"),(\"data\\ya\\input.txt\",\"ya_model\")]\n",
    "np.random.shuffle(text_list)\n",
    "t = text_list[2][0] #THIS TEXT IS THE VOCAB!\n",
    "save_dir = text_list[2][1] #THIS IS THE MODEL DIRECTORY\n",
    "text = open(t)\n",
    "text = text.read()\n",
    "with open(\"./cmudict-0.7b.txt\") as f:\n",
    "    lines = [line.rstrip(\"\\n\").split() for line in f if (\";;;\" not in line)]\n",
    "dictMeters = {}\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i]\n",
    "    newLine = [line[0].lower()]\n",
    "    if(\"(\" in newLine[0] and \")\" in newLine[0]):\n",
    "        newLine[0] = newLine[0][:-3]\n",
    "    chars = \"\"\n",
    "    for word in line[1:]:\n",
    "        for ch in word:\n",
    "            if(ch in \"012\"):\n",
    "                if(ch == \"2\"):\n",
    "                    chars+=\"1\"\n",
    "                else:\n",
    "                    chars+=ch\n",
    "    newLine+=[chars]\n",
    "    lines[i] = newLine\n",
    "    if(newLine[0] not in dictMeters): #THIS IF STATEMENT ALLOWS FOR MULTIPLE PRONUNCIATIONS OF A WORD\n",
    "        dictMeters[newLine[0]]=[chars]\n",
    "    else:\n",
    "        if(chars not in dictMeters[newLine[0]]):\n",
    "            dictMeters[newLine[0]]+=[chars]\n",
    "words = [simple_clean(word) for word in text.split()]\n",
    "uniques = set()\n",
    "for word in words:\n",
    "    if word not in dictMeters:\n",
    "        continue\n",
    "    else:\n",
    "        uniques.add(word)\n",
    "corpus = list(uniques)\n",
    "dictCorpusMeterGroups = createMeterGroups(corpus,dictMeters)\n",
    "dictWordTransitions = {}\n",
    "#fsaLine = [State((0,i)) for i in range(11)]\n",
    "dictMeters[\"i\"]+=[\"0\"]\n",
    "dictMeters[\"the\"]+=[\"0\"]\n",
    "glove_words = glove_model.vocab.keys() #SHOULDN'T THIS HAVE BEEN MOST COMMON WORDS IN THE CORPUS?\n",
    "# CORPUS LIMITED TO WORDS IN GLOVE VOCAB\n",
    "word_counts = collections.Counter([x for x in words if x in dictMeters])\n",
    "corpi = [x[0] for x in word_counts.most_common() if x[0] in glove_words]\n",
    "dictCorpusMeterGroups = createMeterGroups(corpi,dictMeters)\n",
    "dictPartSpeechTags = createPartSpeechTags(corpi,dictMeters)\n",
    "dictPossiblePartsSpeech = possiblePartsSpeechPaths()\n",
    "#fsa_ret = formLinkedTree(0,dictMeters,corpi,fsaLine,dictWordTransitions,dictCorpusMeterGroups)\n",
    "#fsa_row = fsa_ret[0]\n",
    "  ########################\n",
    "from collections import defaultdict\n",
    "POS_terms= defaultdict(list)\n",
    "for word, tag in dictPartSpeechTags.items():\n",
    "    POS_terms[tag].append(word)\n",
    "def get_possible_words(word, POS_terms):\n",
    "    pos=nltk.pos_tag([word])[0][1]\n",
    "    list_POS=dictPossiblePartsSpeech[pos]\n",
    "    res=[]\n",
    "    for x in list_POS:\n",
    "        res.extend(POS_terms[x])\n",
    "    return res\n",
    "###################################### \n",
    "#post_stress=5\n",
    "#pred_stress=5\n",
    "#width = 50\n",
    "#wordPools = [set([]) for n in range(4)]\n",
    "#poem_1= genPoem('grapes', 'purple', save_dir,topic,width,wordPools)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model to be used. DEFAULT=WHITMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "poet=0 # CHANGE HERE FROST=0, WHITMAN=1\n",
    "\n",
    "\n",
    "\n",
    "text_list = [(\"data\\\\frost\\\\input.txt\",\"frost_model_forward\"),(\"data\\whitman\\input.txt\",\"whitman_model\"),(\"data\\ya\\input.txt\",\"ya_model\")]\n",
    "#np.random.shuffle(text_list)\n",
    "t = text_list[poet][0] #THIS TEXT IS THE VOCAB!\n",
    "save_dir = text_list[poet][1] #THIS IS THE MODEL DIRECTORY\n",
    "tf.reset_default_graph()\n",
    "with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
    "    saved_args = cPickle.load(f)\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "    word_keys, vocab = cPickle.load(f)\n",
    "model = Model(saved_args, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_init(word):\n",
    "    postag=nltk.pos_tag([word])[0][1]\n",
    "    if postag in set(['RB','RBR', 'RBS', 'VB', 'VBZ', 'LS','UH','PRP','MD']):\n",
    "        return True\n",
    "    return False\n",
    "def init_word(vocabulary):\n",
    "    init=random.choice(vocabulary)\n",
    "    while bool_init(init):\n",
    "        init=random.choice(vocabulary)\n",
    "    return init\n",
    "\n",
    "def bool_last_word(sentence):\n",
    "    last=nltk.pos_tag(sentence)\n",
    "    last=last[-1][1]\n",
    "    if last not in set(['EX','JJ','JJR','JJS','WDT','WP','WRB','PRP','RP','TO']):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_last(sess, model, vocab, prob_seq, sequence, state, temp, lists, nwords, width, rest_vocab):\n",
    "    d,s=model.compute_fx(sess, vocab, prob_seq, sequence, state, temp)\n",
    "    if len(sequence)<nwords:\n",
    "        new_vocab=[vocab[x] for x in rest_vocab]\n",
    "        d_new=d[new_vocab]\n",
    "        max_ind=d_new.argsort()[-width:][::-1]\n",
    "        for x in max_ind:\n",
    "            new_prob=d[new_vocab[x]]\n",
    "            new_word=list(vocab.keys())[new_vocab[x]]\n",
    "            if(new_word == sequence[-1]):\n",
    "                continue\n",
    "            if(partsOfSpeechFilter(sequence[-1],new_word,dictPartSpeechTags,dictPossiblePartsSpeech)):\n",
    "                continue\n",
    "            generate_last(sess, model, vocab, new_prob, sequence+[new_word],s, temp, lists, nwords, width, rest_vocab)\n",
    "    else:\n",
    "        if bool_last_word(sequence):\n",
    "            result=(prob_seq, sequence)\n",
    "            return lists.append(result)\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertebrates->bird->\u001b[4mfowl\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mperson\u001b[0m->human\n"
     ]
    }
   ],
   "source": [
    "f=print_five_words('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rest_vocab(definition1, definition2, vocabulary):\n",
    "    result=simple_clean(definition1).split()\n",
    "    result2=simple_clean(definition2).split()\n",
    "    final=set(result+result2)\n",
    "    final=list(final)+stopwords.words('english')\n",
    "    return [x for x in final if x in set(list(vocabulary.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create poem by using the synsets\n",
    "\n",
    "#### Inputs: model, word to be used to construct the meta-poem, number of words in line\n",
    "\n",
    "--It creates the 5 words required for the poem\n",
    "\n",
    "-- check if all the words are in the vocab\n",
    "\n",
    "--- Construct the lines by using pairs of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poem_2(model, seed, length_line):\n",
    "    result=[]\n",
    "    five=print_five_words(seed)\n",
    "    five_def=create_pairs([x[1] for x in five])\n",
    "    five_words=create_pairs([x[0] for x in five])\n",
    "    print(' ')\n",
    "    if bool_five_words([x[0] for x in five], vocab)==False:\n",
    "        print ('WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED')\n",
    "        return None\n",
    "    else:\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                state = sess.run(model.initial_state)\n",
    "                init_score = np.array([[0]])\n",
    "                list_pairs=create_pairs(five)\n",
    "                for pair_word, pair_def in zip(five_words, five_def):\n",
    "                    \n",
    "                    tt_list=[]\n",
    "                    res_vocab=build_rest_vocab(pair_def[0], pair_def[1], vocab)\n",
    "                    res_vocab=res_vocab+pair_word\n",
    "                    init=init_word(res_vocab)\n",
    "                    generate_last(sess, model, vocab, init_score, [init], state, 1, tt_list,length_line, 20, res_vocab)\n",
    "                    final_list=sorted(tt_list, key=lambda tup: tup[0], reverse=True)\n",
    "                    result.append(' '.join(sampleLine(final_list, 10)))\n",
    "        print('------POEM------')\n",
    "        for x in result:\n",
    "            print(x)\n",
    "        print('-----------------')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poem(length):\n",
    "    i=0\n",
    "    k=list(vocab.keys())\n",
    "    random.shuffle(k)\n",
    "    k=k[:100]\n",
    "    word=k[i]\n",
    "    try: \n",
    "        t=create_poem_2(model, word, lenth)\n",
    "    except UnboundLocalError or ValueError or TypeError:\n",
    "        t=None\n",
    "    while t==None:\n",
    "        i+=1\n",
    "        word=k[i]\n",
    "        try: \n",
    "            t=create_poem_2(model, word, length)\n",
    "        except UnboundLocalError or ValueError or TypeError:\n",
    "            t=None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group->people->\u001b[4mgenerations\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mstudy\u001b[0m->inspection\n",
      " \n",
      "WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED\n",
      "tendency->elasticity->\u001b[4mgive\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mtest\u001b[0m->something\n",
      " \n",
      "WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED\n",
      "knowledge->awareness->\u001b[4msensation\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mmotion\u001b[0m->use\n",
      " \n",
      "WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED\n",
      "act->prayer->\u001b[4mcollect\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mdevelop\u001b[0m->make\n",
      " \n",
      "WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED\n",
      "scope->expanse->\u001b[4mspace\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mspace\u001b[0m->expanse\n",
      " \n",
      "WORDS NOT IN VOCAB, PLEASE INSERT A DIFFERENT SEED\n",
      "sugar->brittle->\u001b[4mbe\u001b[0m\u001b[1m~~>\u001b[0m\u001b[4mbe\u001b[0m->brittle\n",
      " \n",
      "INFO:tensorflow:Restoring parameters from frost_model_forward\\model.ckpt-80000\n",
      "INFO:tensorflow:Restoring parameters from frost_model_forward\\model.ckpt-80000\n",
      "was\n",
      "i\n",
      "when\n"
     ]
    }
   ],
   "source": [
    "poem(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
