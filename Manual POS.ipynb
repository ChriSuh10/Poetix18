{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "https://sites.google.com/site/partofspeechhelp/home#TOC-IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle if you don't have GPT-2/its dependencies\n",
    "use_gpt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "if use_gpt:\n",
    "    from gpt2.src.encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local version of dicts\n",
    "with open('py_files/saved_objects/postag_dict_all.p', 'rb') as f:\n",
    "    postag = pickle.load(f)\n",
    "    \n",
    "pos_to_words = postag[1]\n",
    "words_to_pos = postag[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads latest copy of words that have been checked\n",
    "checked = set()\n",
    "checked_this_session = set()\n",
    "with open('data/checked_words.txt') as f:\n",
    "    for w in f:\n",
    "        checked.add(w.strip('\\n'))\n",
    "\n",
    "if use_gpt:\n",
    "    enc = get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He snowflakes\n",
      "He worries\n",
      "He curst\n",
      "He persists\n",
      "He todays\n",
      "He provokes\n",
      "He swam\n",
      "He loghouses\n",
      "He swells\n",
      "He turns\n",
      "He unawares\n",
      "He overweighs\n",
      "He weakens\n",
      "He buzz\n",
      "He distills\n",
      "\n",
      "111 words checked this session, 2200 checked overall\n"
     ]
    }
   ],
   "source": [
    "# Edit this so the pos will make sense when appended\n",
    "line = ['He']\n",
    "\n",
    "# run this to get new batch of words\n",
    "num_words = 15\n",
    "\n",
    "### CHANGE THIS FOR YOUR POS ###\n",
    "# POS you will be working on\n",
    "# Chris - VBZ\n",
    "# Andre - VBG\n",
    "# Isaac - JJ\n",
    "# Henry - NNP\n",
    "# Matias - VB\n",
    "# Cynthia - RB\n",
    "pos = 'VBZ'\n",
    "to_print = []\n",
    "for i in range(num_words):\n",
    "    w = random.choice(pos_to_words[pos])\n",
    "    while w in checked or w in checked_this_session or (use_gpt and len(enc.encode(w)) != 1):\n",
    "        w = random.choice(pos_to_words[pos])\n",
    "    checked_this_session.add(w)\n",
    "    this_line = [*line, w]\n",
    "    to_print.append(' '.join(this_line))\n",
    "    \n",
    "for l in to_print:\n",
    "    print(l)\n",
    "print('')\n",
    "print('{:d} words checked this session, {:d} checked overall'.format(len(checked_this_session), \n",
    "                                                                     len(checked_this_session) + len(checked)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNS', 'VBZ']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show pos of words you want to check\n",
    "word = 'unawares'\n",
    "words_to_pos[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pos from word and word from pos\n",
    "remove_pos = ['VBZ', 'NNS']\n",
    "for pos in remove_pos:\n",
    "    words_to_pos[word].remove(pos)\n",
    "    pos_to_words[pos].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pos to word and word to pos\n",
    "add_pos = 'RB'\n",
    "words_to_pos[word].append(add_pos)\n",
    "pos_to_words[add_pos].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the word (for words that don't make sense)\n",
    "for pos in words_to_pos[word]:\n",
    "    try:\n",
    "        pos_to_words[pos].remove(word)\n",
    "    except:\n",
    "        continue\n",
    "_ = words_to_pos.pop(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At end, when you need to commit changes to the dictionary\n",
    "with open('data/checked_words.txt', 'a+') as f:\n",
    "    for w in checked_this_session:\n",
    "        f.write(w)\n",
    "        f.write('\\n')\n",
    "        \n",
    "postag[1] = pos_to_words\n",
    "postag[2] = words_to_pos\n",
    "with open('py_files/saved_objects/postag_dict_all.p', 'wb') as f:\n",
    "    pickle.dump(postag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words that we don't know what to do with\n",
    "# wouldnt, doesnt, aint, heres, ___-___-olds, hes, heres, thats, milan's, ryan's, satan's, today's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Roman numerals as symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym = 'lv'\n",
    "words_to_pos[sym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_words['NN'].remove(sym)\n",
    "words_to_pos[sym] = ['SYM']\n",
    "pos_to_words['SYM'] = sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lv, xc, ii, xx, xi, ll, xxx, ci, iii, li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NLTK to add words to pos dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files.Limericks import Limerick_Generate\n",
    "from nltk.corpus import words\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = Limerick_Generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/isaac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(50257):\n",
    "    word = lg.enc.decode([i]).lower().strip()\n",
    "    if word in word_list:\n",
    "        # Word length constraint because many prefixes and sufixes are in NLTK word list\n",
    "        if len(words_to_pos[word]) == 0 and len(word) >= 4: \n",
    "            words_to_pos[word].append(nltk.pos_tag([word.lower().strip()])[0][1])\n",
    "            count += 1\n",
    "            print(count,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag[2] = words_to_pos\n",
    "with open('py_files/saved_objects/postag_dict_all.p', 'wb') as f:\n",
    "    pickle.dump(postag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG 1784\n",
      "IN 62\n",
      "PRP 25\n",
      "DT 19\n",
      "NN 9717\n",
      "WDT 3\n",
      "VBZ 960\n",
      "VBN 1662\n",
      ", 1\n",
      "RB 1044\n",
      "VBP 1231\n",
      "VB 1619\n",
      "JJ 3851\n",
      "WP 5\n",
      "MD 17\n",
      "TO 1\n",
      "NNP 2877\n",
      "CC 9\n",
      "VBD 1631\n",
      "CD 19\n",
      "NNS 4458\n",
      "PRP$ 12\n",
      "EX 1\n",
      "JJS 222\n",
      "JJR 108\n",
      "RBR 11\n",
      "UH 9\n",
      "FW 21\n",
      "PDT 6\n",
      "WRB 7\n",
      "RP 48\n",
      ". 2\n",
      "WP$ 1\n",
      "RBS 2\n",
      "NNPS 68\n",
      "SYM 2\n",
      "SO 1\n",
      "WHO 1\n",
      "THAN 1\n",
      "AS 1\n",
      "WHEN 1\n",
      "IF 1\n",
      "POS 1\n"
     ]
    }
   ],
   "source": [
    "for k, l in pos_to_words.items():\n",
    "    print(k, len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
