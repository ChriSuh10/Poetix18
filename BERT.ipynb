{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hugging Face.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChriSuh10/Poetix18/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8rKmgaj26mFc",
        "colab_type": "code",
        "outputId": "149957f7-d79b-422d-ef4c-3051aeaf2fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install ftfy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.123)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.123)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxQWh2xKYywt",
        "colab_type": "code",
        "outputId": "f02e8f92-e00e-43b6-de1a-691213fa7c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hVk8rUr-Zt2D",
        "colab_type": "code",
        "outputId": "e644e08a-a8bb-476c-d850-05d636936e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/Poetix/Hugging\\ Face"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Poetix/Hugging Face\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8iTUpQ18tJp",
        "colab_type": "code",
        "outputId": "47211211-7456-4561-c1ae-96bd705bcf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "import itertools"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFY2BnBEBUS7",
        "colab_type": "code",
        "outputId": "3ddf82c0-26ef-4247-f305-4d1673e141db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.to('cuda')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:37<00:00, 10943941.85B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 413934.29B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4OxasrxGBaWj",
        "colab_type": "code",
        "outputId": "cdab7ca2-f2d5-4faa-bb39-31aa9a9cbd3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "def raw_text_to_tokens(text, masks):\n",
        "  tokenized_text = ['[CLS]']\n",
        "  for i, w in enumerate(tokenizer.tokenize(text)):\n",
        "    if i in masks:\n",
        "      w = '[MASK]'\n",
        "    elif w == '.':\n",
        "      w = '[SEP]'\n",
        "    tokenized_text.append(w)\n",
        "  \n",
        "  return tokenized_text\n",
        "\n",
        "def token_segment_tensors(tokens, to_cuda=True):\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  segment_ids = [0 for i in range(len(tokens))]\n",
        "  \n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segment_tensor = torch.tensor([segment_ids])\n",
        "  \n",
        "  if to_cuda:\n",
        "    tokens_tensor = tokens_tensor.to('cuda')\n",
        "    segment_tensor = segment_tensor.to('cuda')\n",
        "  \n",
        "  return tokens_tensor, segment_tensor\n",
        "  \n",
        "def feed_bert(text, masks, no_punct=True):\n",
        "  tokenized_text = raw_text_to_tokens(text, masks)\n",
        "  \n",
        "  tokens_tensor, segment_tensor = token_segment_tensors(tokenized_text)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segment_tensor)\n",
        "  \n",
        "  pred_str = text.split()\n",
        "  if no_punct:\n",
        "    punct_indices = [1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]\n",
        "    top_indices = torch.argsort(predictions, dim=2, descending=True)\n",
        "    for i in masks:\n",
        "      for j in range(len(punct_indices) + 1):\n",
        "        potential_prediction = top_indices[0, i + 1, j].item()\n",
        "        if potential_prediction not in punct_indices:\n",
        "          predicted_token = tokenizer.convert_ids_to_tokens([potential_prediction])[0]\n",
        "          pred_str.pop(i)\n",
        "          pred_str.insert(i, predicted_token)\n",
        "          break\n",
        "  else:\n",
        "    for i in masks:\n",
        "      # Because we add [CLS] to the beginning of the sentence\n",
        "      predicted_index = torch.argmax(predictions[0, i + 1]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      pred_str.pop(i)\n",
        "      pred_str.insert(i, predicted_token)\n",
        "  return pred_str, tokenized_text\n",
        "\n",
        "text = 'There once was a woman from Niger.'\n",
        "masks = [3, 5]\n",
        "\n",
        "pred, masked = feed_bert(text, masks)\n",
        "\n",
        "print(text)\n",
        "print(' '.join(masked))\n",
        "print(' '.join(pred))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There once was a woman from Niger.\n",
            "[CLS] there once was [MASK] woman [MASK] niger [SEP]\n",
            "There once was a woman in Niger.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yh1ZnlzZb-Fu",
        "outputId": "a9535be7-ddb2-492d-dd99-d88dee05f728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def mask_in_range(tokens, masks):\n",
        "  masked_tokens = []\n",
        "  for i in range(len(tokens)):\n",
        "    if i in masks:\n",
        "      masked_tokens.append('[MASK]')\n",
        "    else:\n",
        "      masked_tokens.append(tokens[i])\n",
        "  return masked_tokens\n",
        "    \n",
        "\n",
        "### For sequence (x1, x2, ..., xn),\n",
        "### returns p(x1) * p(x2 | x1) * ... * p(xn | x1, ..., xn-1)\n",
        "def score_sentence_bert(text):\n",
        "  tokenized_text = raw_text_to_tokens(text, [])\n",
        "#   print(tokenized_text)\n",
        "  # compute log probabilities across all words for each token\n",
        "  log_prob = torch.nn.LogSoftmax(dim=2)\n",
        "  prob_by_spot = []\n",
        "  probs = 0\n",
        "  # since sentence begins with '[CLS]'\n",
        "  for i in range(1, len(tokenized_text)):\n",
        "    masked_tokens = tokenized_text[:i]\n",
        "    masked_tokens.append('[MASK]')\n",
        "    tokens_tensor, segment_tensor = token_segment_tensors(masked_tokens)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      log_probs = log_prob(model(tokens_tensor))\n",
        "      this_log_prob = log_probs[0, i, tokens_tensor[0, i]]\n",
        "      probs += this_log_prob.item()\n",
        "      prob_by_spot.append(this_log_prob.item())\n",
        "\n",
        "#   print(prob_by_spot)\n",
        "  return probs / len(tokenized_text)\n",
        "    \n",
        "    \n",
        "\n",
        "human_text = 'Who wanted to become a ballerina'\n",
        "generated_text = 'been one perturbations of his liar'\n",
        "\n",
        "print(score_sentence_bert(human_text))\n",
        "print(score_sentence_bert(generated_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-11.797262072563171\n",
            "-12.830491924285889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JWfQI-HIZSNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data structures\n",
        "with open('templates.p', 'rb') as f:\n",
        "  dataset, second_line_, third_line_, last_two_lines = pickle.load(f)\n",
        "\n",
        "with open('likelihood_collocations.p', 'rb') as f:\n",
        "  coll_dict = pickle.load(f)\n",
        "  \n",
        "# map second word to first word\n",
        "rev_coll_dict = {}\n",
        "for k, v in coll_dict.items():\n",
        "  for w, score in v:\n",
        "    if w not in rev_coll_dict:\n",
        "      rev_coll_dict[w] = []\n",
        "    rev_coll_dict[w].append((k, score))\n",
        "for l in rev_coll_dict.values():\n",
        "  l.sort(key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "with open('postag_dict_all.p', 'rb') as f:\n",
        "  postag_dict = pickle.load(f)\n",
        "\n",
        "pos_to_words = postag_dict[1]\n",
        "words_to_pos = postag_dict[2]\n",
        "\n",
        "# map words to meter\n",
        "with open('cmudict-0.7b.txt') as f:\n",
        "    lines = [line.rstrip(\"\\n\").split() for line in f if (\";;;\" not in line)]\n",
        "    dict_meters = {}\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        newLine = [line[0].lower()]\n",
        "        if(\"(\" in newLine[0] and \")\" in newLine[0]):\n",
        "            newLine[0] = newLine[0][:-3]\n",
        "        chars = \"\"\n",
        "        for word in line[1:]:\n",
        "            for ch in word:\n",
        "                if(ch in \"012\"):\n",
        "                    if(ch == \"2\"):\n",
        "                        chars+=\"1\"\n",
        "                    else:\n",
        "                        chars+=ch\n",
        "        newLine+=[chars]\n",
        "        lines[i] = newLine\n",
        "        if(newLine[0] not in dict_meters): #THIS IF STATEMENT ALLOWS FOR MULTIPLE PRONUNCIATIONS OF A WORD\n",
        "            dict_meters[newLine[0]]=[chars]\n",
        "        else:\n",
        "            if(chars not in dict_meters[newLine[0]]):\n",
        "                dict_meters[newLine[0]]+=[chars]\n",
        "    dict_meters[','] = ['']\n",
        "    dict_meters['.'] = ['']\n",
        "    \n",
        "# map pos to possible syllables\n",
        "pos_syllables = {}\n",
        "for k, v in pos_to_words.items():\n",
        "    pos_syllables[k] = set()\n",
        "    for w in v:\n",
        "        try:\n",
        "            pos_syllables[k].add(len(dict_meters[w][0]))\n",
        "        except:\n",
        "            continue\n",
        "pos_syllables[','].add(0)\n",
        "pos_syllables['.'].add(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMPttlMlpzq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def valid_permutation_sylls(num_sylls, template, last_word_sylls):\n",
        "        \"\"\"\n",
        "        Finds and returns the first integer partition of num_sylls with a total\n",
        "        number of integers equal to the length of template - 1 for which each\n",
        "        assignment of syllables to pos is valid.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_sylls : int\n",
        "            The total number of syllables to be distributed across the words in\n",
        "            the line.\n",
        "        template : list\n",
        "            A list of str containing the pos for each word in the line.\n",
        "        last_word_sylls : int\n",
        "            The number of syllables of the last word in the line.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list\n",
        "            A list of ints corresponding to a valid assignment of syllables to\n",
        "            each word in the line.\n",
        "        \"\"\"\n",
        "        def get_all_partition_size_n(n, partition_size):\n",
        "            \"\"\"\n",
        "            Returns all integer partitions of an int with a partition_size number\n",
        "            of ints.\n",
        "            \"\"\"\n",
        "            def get_all_partitions(n, I=1):\n",
        "                yield (n,)\n",
        "                for i in range(I, n//2 + 1):\n",
        "                    for p in get_all_partitions(n-i, i):\n",
        "                        yield (i,) + p\n",
        "            return [p for p in get_all_partitions(n) if len(p) == partition_size]\n",
        "        def valid_syll(sylls, template):\n",
        "            \"\"\"\n",
        "            Checks if a template and syllable mapping are compatible.\n",
        "            \"\"\"\n",
        "            for i in range(len(template) - 1):\n",
        "                # Add in zeros to account for punctuation\n",
        "                if template[i] == ',' or template[i] == '.':\n",
        "                    sylls.insert(i, 0)\n",
        "                if sylls[i] not in pos_syllables[template[i]]:\n",
        "                    return False\n",
        "            return True\n",
        "        syllables_left = num_sylls - last_word_sylls\n",
        "        # Punctuation takes up no syllables, so subtract to get number of partitions\n",
        "        num_zero_sylls = sum(1 if pos == '.' or pos == ',' else 0 for pos in template)\n",
        "        num_words_left = len(template) - num_zero_sylls - 1\n",
        "\n",
        "        for partition in get_all_partition_size_n(syllables_left, num_words_left):\n",
        "            # Goes through all permutations by index, not numbers,\n",
        "            # inefficient implementation\n",
        "            permutations = list(itertools.permutations(partition))\n",
        "            random.shuffle(permutations)\n",
        "            for perm in permutations:\n",
        "                perm = list(perm)\n",
        "                # Last word is fixed\n",
        "                perm.append(last_word_sylls)\n",
        "                if valid_syll(perm, template):\n",
        "                    return perm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LcxYDpfEdj-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_colls_to_line(i, sequence, template, template_sylls):\n",
        "  curr_word = sequence[i]\n",
        "  if curr_word == '[MASK]':\n",
        "    return\n",
        "  colls = rev_coll_dict[curr_word]\n",
        "  for j in reversed(range(max(0, i - 3), i)):\n",
        "    for coll_word, score in colls:\n",
        "      if sequence[j] == '[MASK]' and coll_word not in sequence and template[j] in words_to_pos[coll_word] and coll_word in dict_meters and template_sylls[j] == len(dict_meters[coll_word][0]):\n",
        "        sequence[j] = coll_word\n",
        "\n",
        "def fill_from_back(sequence, template, template_sylls):\n",
        "  for i in reversed(range(len(sequence))):\n",
        "      add_colls_to_line(i, sequence, template, template_sylls)\n",
        "      \n",
        "def feed_bert_templates(text, masks, template, template_syll, no_punct=True):\n",
        "  tokenized_text = raw_text_to_tokens(text, masks)\n",
        "  \n",
        "  tokens_tensor, segment_tensor = token_segment_tensors(tokenized_text)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segment_tensor)\n",
        "  \n",
        "  pred_str = text.split()\n",
        "  if no_punct:\n",
        "    punct_indices = [1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]\n",
        "    top_indices = torch.argsort(predictions, dim=2, descending=True)\n",
        "    for i in masks:\n",
        "      for j in range(len(top_indices[0, i + 1])):\n",
        "        potential_prediction = top_indices[0, i + 1, j].item()\n",
        "        if potential_prediction not in punct_indices:\n",
        "          predicted_token = tokenizer.convert_ids_to_tokens([potential_prediction])[0]\n",
        "          if not (predicted_token in words_to_pos and predicted_token in dict_meters):\n",
        "            continue\n",
        "          if template[i] in words_to_pos[predicted_token] and len(dict_meters[predicted_token][0]) == template_syll[i]:\n",
        "            pred_str.pop(i)\n",
        "            pred_str.insert(i, predicted_token)\n",
        "            break\n",
        "  else:\n",
        "    for i in masks:\n",
        "      # Because we add [CLS] to the beginning of the sentence\n",
        "      predicted_index = torch.argmax(predictions[0, i + 1]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      pred_str.pop(i)\n",
        "      pred_str.insert(i, predicted_token)\n",
        "  return pred_str, tokenized_text\n",
        "\n",
        "def try_all_low_freq_pos(w1, template, template_sylls):\n",
        "  # iterate over all permutions of low pos words\n",
        "  return\n",
        "\n",
        "def get_template(pos, line_number, num_templates=1):\n",
        "  if line_number == 1:\n",
        "    return\n",
        "  if line_number == 2:\n",
        "    templates = random.sample(second_line_[pos], k=num_templates)\n",
        "  if line_number == 3:\n",
        "    templates = random.sample(third_line_[pos], k=num_templates)\n",
        "  if line_number == 4:\n",
        "    templates = random.sample(last_two_lines[pos], k=num_templates)\n",
        "  return templates\n",
        "\n",
        "def gen_line(w1, line=2, template=None, num_sylls=10):\n",
        "  pos = words_to_pos[w1][0]\n",
        "  if template is None:\n",
        "    template, original = get_template(pos, line)\n",
        "  \n",
        "  last_word_sylls = len(dict_meters[w1][0])\n",
        "  template_sylls = valid_permutation_sylls(num_sylls, template, last_word_sylls)\n",
        "  if template_sylls is None:\n",
        "    print(num_sylls, w1)\n",
        "  \n",
        "  seq = ['[MASK]' for i in range(len(template_sylls) - 1)]\n",
        "  seq.append(w1)\n",
        "  fill_from_back(seq, template, template_sylls)\n",
        "  \n",
        "  masks = [i for i in range(len(seq)) if seq[i] == '[MASK]']\n",
        "  as_str = ' '.join(seq)\n",
        "  print(as_str)\n",
        "  pred_seq, masked_tokens = feed_bert_templates(as_str, masks, template, template_sylls)\n",
        "  \n",
        "  for iteration in range(10):\n",
        "    if '[MASK]' in pred_seq:\n",
        "      masks = [i for i in range(len(seq)) if seq[i] == '[MASK]']\n",
        "      as_str = ' '.join(pred_seq)\n",
        "      pred_seq, masked_tokens = feed_bert_templates(as_str, masks, template, template_sylls)\n",
        "    else:\n",
        "      break\n",
        "  score = score_sentence_bert(' '.join(pred_seq))\n",
        "  \n",
        "  return pred_seq, score, template"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OiKlPyjrlpno",
        "colab_type": "code",
        "outputId": "74df1c09-cd98-4326-9b81-cb65b77ecb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "seq = ['[MASK]', '[MASK]', '[MASK]', '[MASK]', 'amount']\n",
        "template = ['JJ', 'VB', 'JJ', 'VB', 'NN']\n",
        "template_sylls = [1, 1, 1, 1, 2]\n",
        "\n",
        "add_colls_to_line(4, seq, template, template_sylls)\n",
        "print(seq)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[MASK]', '[MASK]', 'fair', '[MASK]', 'amount']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xFCohf4YplMZ",
        "colab_type": "code",
        "outputId": "4da738b9-df75-449e-ffdc-255e28047b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "gen_line('amount')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MASK] [MASK] [MASK] [MASK] flaw [MASK] [MASK] amount\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['who', 'remained', 'the', 'new', 'flaw', 'in', 'its', 'amount'],\n",
              " -12.3067594104343,\n",
              " ['WHO', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "qokKWopCAgtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ffb690e8-2135-4132-cf0e-f1d36aaec84e"
      },
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "for k, v in tokenizer.wordpiece_tokenizer.vocab.items():\n",
        "  if k in words_to_pos:\n",
        "    s += 1\n",
        "print(s)\n",
        "print(s / len(words_to_pos))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11449\n",
            "0.44475953694351644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NmZTORrSenZO",
        "colab_type": "code",
        "outputId": "2cfc8b26-23ff-4d11-c4e4-b2a6e50b95cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "cell_type": "code",
      "source": [
        "def gen_poem(five_words):\n",
        "  templates = []\n",
        "  first_pos = words_to_pos[five_words[0]][0]\n",
        "  templates.append(get_template(first_pos, 2)[0][0])\n",
        "  second_pos = words_to_pos[five_words[1]][0]\n",
        "  templates.append(get_template(second_pos, 2)[0][0])\n",
        "  third_pos = words_to_pos[five_words[2]][0]\n",
        "  templates.append(get_template(third_pos, 3)[0][0])\n",
        "  fourth_pos = words_to_pos[five_words[3]][0]\n",
        "  fifth_pos = words_to_pos[five_words[4]][0]\n",
        "  last_template, original, idx = get_template(fourth_pos + \"-\" + fifth_pos, 4)[0]\n",
        "  fourth_template = last_template[:idx + 1]\n",
        "  fifth_template = last_template[idx + 1:]\n",
        "  templates.extend((fourth_template, fifth_template))\n",
        "  \n",
        "#   print(templates)\n",
        "  \n",
        "  lines = []\n",
        "  syllables = [len(t) + len(dict_meters[five_words[i]][0]) + 1 for i, t in enumerate(templates)]\n",
        "  for i, word in enumerate(five_words):\n",
        "    lines.append(gen_line(five_words[i], template=templates[i], num_sylls=syllables[i]))\n",
        "    \n",
        "  return lines\n",
        "\n",
        "def print_poem(five_words):         \n",
        "  out = gen_poem(five_words)\n",
        "  print(\"*********************\")\n",
        "  for l, s, t in out:\n",
        "    print('{:60} line score: {:2.3f}'.format(' '.join(l), s))\n",
        "    print(t)\n",
        "\n",
        "print_poem(('sire', 'fire', 'kettle', 'metal', 'wire'))\n",
        "print_poem(('birthplace', 'grace', 'report', 'court', 'case'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] sire\n",
            "[MASK] dimmed [MASK] [MASK] recovered [MASK] [MASK] fire\n",
            "[MASK] cook [MASK] soup kettle\n",
            "[MASK] [MASK] [MASK] small metal\n",
            "[MASK] [MASK] [MASK] curl [MASK] coil [MASK] [MASK] wire\n",
            "*********************\n",
            "who was overthrow not of the sire                            line score: -12.102\n",
            "['WHO', 'VBD', 'PRP', 'RB', 'IN', 'DT', 'NN']\n",
            "who dimmed as it recovered in the fire                       line score: -12.967\n",
            "['WHO', 'VBD', 'AS', 'PRP', 'VBD', 'IN', 'DT', 'NN']\n",
            "she cook another soup kettle                                 line score: -11.652\n",
            "['PRP', 'VBP', 'DT', 'NN', 'NN']\n",
            "i included one small metal                                   line score: -11.724\n",
            "['PRP', 'VBD', 'CD', 'JJ', 'NN']\n",
            "in another new curl of coil of her wire                      line score: -12.501\n",
            "['IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'PRP$', 'NN']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-77fae79b9e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sire'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fire'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kettle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wire'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'birthplace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'court'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'case'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-77fae79b9e11>\u001b[0m in \u001b[0;36mprint_poem\u001b[0;34m(five_words)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfive_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfive_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*********************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-77fae79b9e11>\u001b[0m in \u001b[0;36mgen_poem\u001b[0;34m(five_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtemplates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mfirst_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfive_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtemplates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msecond_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfive_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtemplates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-2b8e92b00bbc>\u001b[0m in \u001b[0;36mget_template\u001b[0;34m(pos, line_number, num_templates)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mline_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtemplates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_line_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_templates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mline_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtemplates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthird_line_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_templates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NNP'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "U0OzTAddAen5",
        "colab_type": "code",
        "outputId": "fe89f636-76a4-42f2-d47e-389d4af34412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "poetic_lines = [\"There was an Old Man with a beard Who said It is just as I feared\",\n",
        "                \"Two Owls and a Hen Four Larks and a Wren\",\n",
        "                \"Have all built their nests in my beard\",\n",
        "               \"There was a Young Lady of Dorking Who bought a large bonnet for walking\",\n",
        "                \"But its colour and size So bedazzled her eyes\",\n",
        "                \"That she very soon went back to Dorking\"]\n",
        "\n",
        "generated_lines = [\"who at years deserted last night on sky\",\n",
        "                   \"told me for what no birth if i heard the blue earth\",\n",
        "                   \"the extraordinary way you lie\",\n",
        "                  \"of the whole thing everything alternate who fascinated me now as a mate\",\n",
        "                   \"i continued , with a dream undertone eyes and the team\",\n",
        "                   \"cant sing the fascination of her state\"]\n",
        "\n",
        "poetic_scores = []\n",
        "for line in poetic_lines:\n",
        "  poetic_scores.append(score_sentence_bert(line))\n",
        "  print(line)\n",
        "  \n",
        "generated_scores = []\n",
        "for line in generated_lines:\n",
        "  generated_scores.append(score_sentence_bert(line))\n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'there', 'was', 'an', 'old', 'man', 'with', 'a', 'beard', 'who', 'said', 'it', 'is', 'just', 'as', 'i', 'feared']\n",
            "[-15.141115188598633, -12.286706924438477, -12.675008773803711, -13.918147087097168, -13.837422370910645, -14.138650894165039, -14.68960952758789, -14.954139709472656, -14.437015533447266, -14.79336166381836, -13.459794998168945, -13.66992473602295, -13.885150909423828, -14.508549690246582, -13.66031551361084, -14.599510192871094]\n",
            "There was an Old Man with a beard Who said It is just as I feared\n",
            "['[CLS]', 'two', 'owls', 'and', 'a', 'hen', 'four', 'lark', '##s', 'and', 'a', 'wren']\n",
            "[-15.141115188598633, -14.01447868347168, -14.642720222473145, -14.090720176696777, -13.978071212768555, -13.346451759338379, -14.218205451965332, -15.601720809936523, -13.77589225769043, -14.648971557617188, -13.974457740783691]\n",
            "Two Owls and a Hen Four Larks and a Wren\n",
            "['[CLS]', 'have', 'all', 'built', 'their', 'nests', 'in', 'my', 'beard']\n",
            "[-15.141115188598633, -13.587356567382812, -13.417247772216797, -13.32610034942627, -13.881568908691406, -13.862619400024414, -14.177062034606934, -15.390162467956543]\n",
            "Have all built their nests in my beard\n",
            "['[CLS]', 'there', 'was', 'a', 'young', 'lady', 'of', 'do', '##rkin', '##g', 'who', 'bought', 'a', 'large', 'bonnet', 'for', 'walking']\n",
            "[-15.141115188598633, -12.286706924438477, -12.675008773803711, -14.104532241821289, -13.652532577514648, -13.942829132080078, -14.335205078125, -13.55866527557373, -12.095454216003418, -13.417795181274414, -14.02922248840332, -14.933082580566406, -14.380844116210938, -14.335196495056152, -13.649181365966797, -14.783445358276367]\n",
            "There was a Young Lady of Dorking Who bought a large bonnet for walking\n",
            "['[CLS]', 'but', 'its', 'colour', 'and', 'size', 'so', 'bed', '##az', '##zle', '##d', 'her', 'eyes']\n",
            "[-15.141115188598633, -13.536195755004883, -13.30975341796875, -13.925684928894043, -12.908093452453613, -13.721887588500977, -13.41691780090332, -13.404041290283203, -12.412331581115723, -12.83442497253418, -12.383883476257324, -13.118358612060547]\n",
            "But its colour and size So bedazzled her eyes\n",
            "['[CLS]', 'that', 'she', 'very', 'soon', 'went', 'back', 'to', 'do', '##rkin', '##g']\n",
            "[-15.141115188598633, -12.760953903198242, -14.570377349853516, -14.832463264465332, -14.146479606628418, -13.865888595581055, -14.909444808959961, -14.26722526550293, -14.185141563415527, -12.927114486694336]\n",
            "That she very soon went back to Dorking\n",
            "['[CLS]', 'who', 'at', 'years', 'deserted', 'last', 'night', 'on', 'sky']\n",
            "[-15.141115188598633, -13.862958908081055, -14.177719116210938, -12.988092422485352, -13.235183715820312, -13.993660926818848, -13.88113021850586, -13.996922492980957]\n",
            "who at years deserted last night on sky\n",
            "['[CLS]', 'told', 'me', 'for', 'what', 'no', 'birth', 'if', 'i', 'heard', 'the', 'blue', 'earth']\n",
            "[-15.141115188598633, -13.194348335266113, -13.49009895324707, -14.423521041870117, -12.878868103027344, -13.188740730285645, -13.806846618652344, -14.990436553955078, -14.400728225708008, -13.458734512329102, -14.584299087524414, -13.46316146850586]\n",
            "told me for what no birth if i heard the blue earth\n",
            "['[CLS]', 'the', 'extraordinary', 'way', 'you', 'lie']\n",
            "[-15.141115188598633, -13.953408241271973, -13.016860961914062, -13.635793685913086, -14.18028736114502]\n",
            "the extraordinary way you lie\n",
            "['[CLS]', 'of', 'the', 'whole', 'thing', 'everything', 'alternate', 'who', 'fascinated', 'me', 'now', 'as', 'a', 'mate']\n",
            "[-15.141115188598633, -12.87812328338623, -13.669767379760742, -12.460668563842773, -13.29180908203125, -13.926616668701172, -14.199039459228516, -14.114892959594727, -13.662461280822754, -14.006479263305664, -14.28254508972168, -14.58117961883545, -13.719746589660645]\n",
            "of the whole thing everything alternate who fascinated me now as a mate\n",
            "['[CLS]', 'i', 'continued', ',', 'with', 'a', 'dream', 'under', '##tone', 'eyes', 'and', 'the', 'team']\n",
            "[-15.141115188598633, -12.397459030151367, -14.101753234863281, -14.503713607788086, -14.395990371704102, -14.170354843139648, -13.970682144165039, -14.352154731750488, -14.623027801513672, -13.906976699829102, -15.28161907196045, -15.50830078125]\n",
            "i continued , with a dream undertone eyes and the team\n",
            "['[CLS]', 'can', '##t', 'sing', 'the', 'fascination', 'of', 'her', 'state']\n",
            "[-15.141115188598633, -14.511663436889648, -13.510708808898926, -13.768266677856445, -14.28079605102539, -12.883865356445312, -14.11760139465332, -14.110452651977539]\n",
            "cant sing the fascination of her state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QibK8Mm5DZ62",
        "colab_type": "code",
        "outputId": "66c10da2-fcc0-4657-ef4e-ffcacb407303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sum(generated_scores) / len(generated_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12.576219562985424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "R_zl8tXxk7UY",
        "colab_type": "code",
        "outputId": "5d2203c7-3547-4d55-d991-fc0ee99ec17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sum(poetic_scores) / len(poetic_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12.84572671754764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "gVngz7ycUYVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def possiblePartsSpeechPaths():\n",
        "    #SO dict[\"verb\"] == set(adverb, noun, ...) BUT NOT set(adjective, determiner, etc)\n",
        "    pos_list = [\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\", \"LS\",\"MD\",\"NN\",\"NNS\",\"NNP\",\"NNPS\", \\\n",
        "                \"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\", \\\n",
        "                \"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\",',']\n",
        "    dictTags = {}\n",
        "    for tag in pos_list:\n",
        "        s = set([])\n",
        "        if(\"VB\" in tag):\n",
        "            s = set([\"CC\",\"RB\",\"RBR\",\"RBS\",\"NN\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"MD\",\"PRP\"])\n",
        "            sing_nouns = set([\"NN\",\"NNP\"])\n",
        "            plur_nouns = set([\"NNS\",\"NNPS\"])\n",
        "            if(tag in set([\"VB\",\"VBG\",\"VBP\",\"VBN\"])):\n",
        "                s.difference(sing_nouns)\n",
        "            if(tag in set([\"VBG\",\"VBZ\",\"VBN\"])):\n",
        "                s.difference(plur_nouns)\n",
        "            if(tag in set([\"VBG\",\"VBN\"])):\n",
        "                s.union(set([\"VB\",\"VBD\",\"VBP\",\"VBZ\"]))\n",
        "        else:\n",
        "            s=set(pos_list)\n",
        "            if(\"IN\"==tag):\n",
        "                t = set([\"IN\",\"DT\",\"CC\"]) #maybe not CC\n",
        "                s.difference(t)\n",
        "            if(\"JJ\" in tag):\n",
        "                t = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\"])\n",
        "                s.difference(t)\n",
        "            if(\"TO\"==tag):\n",
        "                t = set([\"DT\",\"CC\",\"IN\"])\n",
        "                s.difference(t)\n",
        "            if(\"CC\"==tag):\n",
        "                t = set([\"DT\",\"JJ\",\"JJR\",\"JJS\"])\n",
        "                s.difference(t)\n",
        "            if(\"NN\" in tag):\n",
        "                t = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"PRP\",\"CC\"]) #maybe not CC\n",
        "                s.difference(t)\n",
        "            if(\"MD\"==tag):\n",
        "                t = set([\"DT\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"])\n",
        "                s.difference(t)\n",
        "            if(\"PRP\"==tag):\n",
        "                t = set([\"CC\",\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"DT\"])\n",
        "                s.difference(t)\n",
        "            if(\"PRP$\"==tag):\n",
        "                t = set([\"CC\",\"DT\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"PRP\"])\n",
        "                s.difference(t)\n",
        "            adv = set([\"RB\",\"RBR\",\"RBS\"])\n",
        "            if(tag not in adv):\n",
        "                s.remove(tag)\n",
        "        dictTags[tag] = s\n",
        "    return dictTags\n",
        "\n",
        "dictPossiblePOS = possiblePartsSpeechPaths()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dnKj5irHQe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partsOfSpeechFilter(sequence, index, word):\n",
        "    if index == len(sequence) - 1 or sequence[index + 1] == '[MASK]':\n",
        "      return False\n",
        "    post_word = sequence[index + 1]\n",
        "    try:\n",
        "        tag1 = set(words_to_pos[post_word])\n",
        "    except KeyError:\n",
        "        return True\n",
        "    try:\n",
        "        tag2 = set(words_to_pos[word])\n",
        "    except KeyError:\n",
        "        return True\n",
        "    #if(tag1==tag2 and tag1 not in okay_tags):\n",
        "    #    return True\n",
        "    \n",
        "    for post_pos in tag1:\n",
        "      if len(tag2.intersection(set(dictPossiblePOS[post_pos]))) > 0:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def get_a_partition_of_n(n, last_word_syll, min_length):\n",
        "    \"\"\"\n",
        "    Returns all integer partitions of an int with a partition_size number\n",
        "    of ints.\n",
        "    \"\"\"\n",
        "    def get_all_partitions(n, I=1):\n",
        "        yield (n,)\n",
        "        for i in range(I, n//2 + 1):\n",
        "            for p in get_all_partitions(n-i, i):\n",
        "                yield (i,) + p\n",
        "    all_possible = [p for p in get_all_partitions(n) if p[-1] == last_word_syll and len(p) >= min_length]\n",
        "    random.shuffle(all_possible)\n",
        "    return all_possible[0]\n",
        "          \n",
        "                  \n",
        "def add_colls_to_line(i, sequence, template_sylls):\n",
        "  curr_word = sequence[i]\n",
        "  if curr_word == '[MASK]':\n",
        "    return\n",
        "  colls = rev_coll_dict[curr_word]\n",
        "  for j in reversed(range(max(0, i - 3), i)):\n",
        "    for coll_word, score in colls:\n",
        "      if sequence[j] == '[MASK]' and coll_word not in sequence and coll_word in dict_meters and template_sylls[j] == len(dict_meters[coll_word][0]):\n",
        "        sequence[j] = coll_word\n",
        "\n",
        "def fill_from_back(sequence, template_sylls):\n",
        "  for i in reversed(range(len(sequence))):\n",
        "      add_colls_to_line(i, sequence, template_sylls)\n",
        "      \n",
        "def feed_bert(text, masks, template_syll, no_punct=True):\n",
        "  tokenized_text = raw_text_to_tokens(text, masks)\n",
        "  \n",
        "  tokens_tensor, segment_tensor = token_segment_tensors(tokenized_text)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segment_tensor)\n",
        "  \n",
        "  pred_str = text.split()\n",
        "  if no_punct:\n",
        "    punct_indices = [1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]\n",
        "    top_indices = torch.argsort(predictions, dim=2, descending=True)\n",
        "    for i in reversed(masks):\n",
        "      print(i)\n",
        "      for j in range(len(top_indices[0, i + 1])):\n",
        "        potential_prediction = top_indices[0, i + 1, j].item()\n",
        "        if potential_prediction not in punct_indices:\n",
        "          predicted_token = tokenizer.convert_ids_to_tokens([potential_prediction])[0]\n",
        "          if predicted_token not in dict_meters:\n",
        "            continue\n",
        "          if predicted_token not in pred_str and len(dict_meters[predicted_token][0]) == template_syll[i] and not partsOfSpeechFilter(pred_str, i, predicted_token):\n",
        "            pred_str.pop(i)\n",
        "            pred_str.insert(i, predicted_token)\n",
        "            break\n",
        "  else:\n",
        "    for i in masks:\n",
        "      # Because we add [CLS] to the beginning of the sentence\n",
        "      predicted_index = torch.argmax(predictions[0, i + 1]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      pred_str.pop(i)\n",
        "      pred_str.insert(i, predicted_token)\n",
        "  return pred_str, tokenized_text\n",
        "\n",
        "def gen_line(w1, num_sylls=10):\n",
        "  last_word_sylls = len(dict_meters[w1][0])\n",
        "  template_sylls = get_a_partition_of_n(num_sylls, last_word_sylls, 4)\n",
        "  print(template_sylls)\n",
        "  \n",
        "  seq = ['[MASK]' for i in range(len(template_sylls) - 1)]\n",
        "  seq.append(w1)\n",
        "#   fill_from_back(seq, template_sylls)\n",
        "  \n",
        "  masks = [i for i in range(len(seq)) if seq[i] == '[MASK]']\n",
        "  as_str = ' '.join(seq)\n",
        "  print(as_str)\n",
        "  pred_seq, masked_tokens = feed_bert(as_str, masks, template_sylls)\n",
        "  score = score_sentence_bert(' '.join(pred_seq))\n",
        "  \n",
        "  return pred_seq, score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l6rh9MZiJoeK",
        "colab_type": "code",
        "outputId": "aa5dad7f-2ddf-49f3-854e-f7d0d8382e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "gen_line('amount', num_sylls=7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2, 2, 2)\n",
            "[MASK] [MASK] [MASK] amount\n",
            "2\n",
            "1\n",
            "0\n",
            "['[CLS]', 'is', 'again', 'also', 'amount']\n",
            "[-15.141115188598633, -14.059334754943848, -12.64249324798584, -13.567127227783203]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['is', 'again', 'also', 'amount'], -11.082014083862305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "biymnRFNW_bj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GPT Experiments"
      ]
    },
    {
      "metadata": {
        "id": "yE0w7-x0xdD9",
        "colab_type": "code",
        "outputId": "2dd34b85-c695-4d0e-cb47-77b13efa3a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2737
        }
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 12407131.58B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 8036751.99B/s]\n",
            "100%|██████████| 548118077/548118077 [00:11<00:00, 46582573.99B/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 31333.99B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): GPT2LMHead(\n",
              "    (decoder): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "W7gN4_Bj6mv1",
        "colab_type": "code",
        "outputId": "55d4b84a-3d68-437f-d22b-f3b54fb7cae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def feed_gpt(text):\n",
        "  indexed_tokens = tokenizer.encode(text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  \n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  model.to('cuda')\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions, past = model(tokens_tensor)\n",
        "    \n",
        "  predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "  predicted_token = tokenizer.decode([predicted_index])\n",
        "  \n",
        "  return predicted_token\n",
        "\n",
        "text = 'Who was Jim Henson? Why was he'\n",
        "feed_gpt(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' so'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "ARjQ2a7X7p1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}