{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hugging Face.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChriSuh10/Poetix18/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8rKmgaj26mFc",
        "colab_type": "code",
        "outputId": "ba6b37ad-2b24-41a5-f3be-4533c68f1797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install ftfy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.123)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.123)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lxQWh2xKYywt",
        "colab_type": "code",
        "outputId": "f60d316b-ca23-4797-ec07-2d87500fc6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hVk8rUr-Zt2D",
        "colab_type": "code",
        "outputId": "928fadf1-2829-4a06-cd19-343adf151f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/Poetix/Hugging\\ Face"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Poetix/Hugging Face\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8iTUpQ18tJp",
        "colab_type": "code",
        "outputId": "873315c7-5567-4c95-af27-da4b63b4f8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "import itertools"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFY2BnBEBUS7",
        "colab_type": "code",
        "outputId": "2e408f2f-c300-460c-803d-8153be293f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.to('cuda')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 48101116.30B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 5683834.57B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4OxasrxGBaWj",
        "colab_type": "code",
        "outputId": "724365d8-5f0e-43d6-cee1-d02c75362e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "def raw_text_to_tokens(text, masks):\n",
        "  tokenized_text = ['[CLS]']\n",
        "  for i, w in enumerate(tokenizer.tokenize(text)):\n",
        "    if i in masks:\n",
        "      w = '[MASK]'\n",
        "    elif w == '.':\n",
        "      w = '[SEP]'\n",
        "    tokenized_text.append(w)\n",
        "  \n",
        "  return tokenized_text\n",
        "\n",
        "def token_segment_tensors(tokens, to_cuda=True):\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  segment_ids = [0 for i in range(len(tokens))]\n",
        "  \n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segment_tensor = torch.tensor([segment_ids])\n",
        "  \n",
        "  if to_cuda:\n",
        "    tokens_tensor = tokens_tensor.to('cuda')\n",
        "    segment_tensor = segment_tensor.to('cuda')\n",
        "  \n",
        "  return tokens_tensor, segment_tensor\n",
        "  \n",
        "def feed_bert(text, masks, no_punct=True):\n",
        "  tokenized_text = raw_text_to_tokens(text, masks)\n",
        "  \n",
        "  tokens_tensor, segment_tensor = token_segment_tensors(tokenized_text)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segment_tensor)\n",
        "  \n",
        "  pred_str = text.split()\n",
        "  if no_punct:\n",
        "    punct_indices = [1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]\n",
        "    top_indices = torch.argsort(predictions, dim=2, descending=True)\n",
        "    for i in masks:\n",
        "      for j in range(len(punct_indices) + 1):\n",
        "        potential_prediction = top_indices[0, i + 1, j].item()\n",
        "        if potential_prediction not in punct_indices:\n",
        "          predicted_token = tokenizer.convert_ids_to_tokens([potential_prediction])[0]\n",
        "          pred_str.pop(i)\n",
        "          pred_str.insert(i, predicted_token)\n",
        "          break\n",
        "  else:\n",
        "    for i in masks:\n",
        "      # Because we add [CLS] to the beginning of the sentence\n",
        "      predicted_index = torch.argmax(predictions[0, i + 1]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      pred_str.pop(i)\n",
        "      pred_str.insert(i, predicted_token)\n",
        "  return pred_str, tokenized_text\n",
        "\n",
        "text = 'There once was a woman from Niger.'\n",
        "masks = [3, 5]\n",
        "\n",
        "pred, masked = feed_bert(text, masks)\n",
        "\n",
        "print(text)\n",
        "print(' '.join(masked))\n",
        "print(' '.join(pred))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There once was a woman from Niger.\n",
            "[CLS] there once was [MASK] woman [MASK] niger [SEP]\n",
            "There once was a woman in Niger.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yh1ZnlzZb-Fu",
        "outputId": "cc510614-4ff5-4c0e-e4f2-652b76295aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "def mask_in_range(tokens, masks):\n",
        "  masked_tokens = []\n",
        "  for i in range(len(tokens)):\n",
        "    if i in masks:\n",
        "      masked_tokens.append('[MASK]')\n",
        "    else:\n",
        "      masked_tokens.append(tokens[i])\n",
        "  return masked_tokens\n",
        "    \n",
        "\n",
        "### For sequence (x1, x2, ..., xn),\n",
        "### returns p(x1) * p(x2 | x1) * ... * p(xn | x1, ..., xn-1)\n",
        "def score_sentence_bert(text):\n",
        "  tokenized_text = raw_text_to_tokens(text, [])\n",
        "  print(tokenized_text)\n",
        "  # compute log probabilities across all words for each token\n",
        "  log_prob = torch.nn.LogSoftmax(dim=2)\n",
        "  prob_by_spot = []\n",
        "  probs = 0\n",
        "  # since sentence begins with '[CLS]'\n",
        "  for i in range(1, len(tokenized_text)):\n",
        "    masked_tokens = tokenized_text[:i]\n",
        "    masked_tokens.append('[MASK]')\n",
        "    tokens_tensor, segment_tensor = token_segment_tensors(masked_tokens)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      log_probs = log_prob(model(tokens_tensor))\n",
        "      this_log_prob = log_probs[0, i, tokens_tensor[0, i]]\n",
        "      probs += this_log_prob.item()\n",
        "      prob_by_spot.append(this_log_prob.item())\n",
        "\n",
        "  print(prob_by_spot)\n",
        "  return probs / len(tokenized_text)\n",
        "    \n",
        "    \n",
        "\n",
        "human_text = 'Who wanted to become a ballerina'\n",
        "generated_text = 'been one perturbations of his liar'\n",
        "\n",
        "print(score_sentence_bert(human_text))\n",
        "print(score_sentence_bert(generated_text))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'who', 'wanted', 'to', 'become', 'a', 'ball', '##erina']\n",
            "[-15.141115188598633, -13.862958908081055, -12.713558197021484, -12.620115280151367, -13.245141983032227, -13.809550285339355, -12.98565673828125]\n",
            "-11.797262072563171\n",
            "['[CLS]', 'been', 'one', 'per', '##tur', '##bation', '##s', 'of', 'his', 'liar']\n",
            "[-15.141115188598633, -13.891103744506836, -14.274194717407227, -14.776897430419922, -13.966140747070312, -14.417414665222168, -14.155193328857422, -13.881111145019531, -13.801748275756836]\n",
            "-12.830491924285889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JWfQI-HIZSNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data structures\n",
        "with open('templates.p', 'rb') as f:\n",
        "  dataset, second_line_, third_line_, last_two_lines = pickle.load(f)\n",
        "\n",
        "with open('likelihood_collocations.p', 'rb') as f:\n",
        "  coll_dict = pickle.load(f)\n",
        "  \n",
        "# map second word to first word\n",
        "rev_coll_dict = {}\n",
        "for k, v in coll_dict.items():\n",
        "  for w, score in v:\n",
        "    if w not in rev_coll_dict:\n",
        "      rev_coll_dict[w] = []\n",
        "    rev_coll_dict[w].append((k, score))\n",
        "for l in rev_coll_dict.values():\n",
        "  l.sort(key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "with open('postag_dict_all.p', 'rb') as f:\n",
        "  postag_dict = pickle.load(f)\n",
        "\n",
        "pos_to_words = postag_dict[1]\n",
        "words_to_pos = postag_dict[2]\n",
        "\n",
        "# map words to meter\n",
        "with open('cmudict-0.7b.txt') as f:\n",
        "    lines = [line.rstrip(\"\\n\").split() for line in f if (\";;;\" not in line)]\n",
        "    dict_meters = {}\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        newLine = [line[0].lower()]\n",
        "        if(\"(\" in newLine[0] and \")\" in newLine[0]):\n",
        "            newLine[0] = newLine[0][:-3]\n",
        "        chars = \"\"\n",
        "        for word in line[1:]:\n",
        "            for ch in word:\n",
        "                if(ch in \"012\"):\n",
        "                    if(ch == \"2\"):\n",
        "                        chars+=\"1\"\n",
        "                    else:\n",
        "                        chars+=ch\n",
        "        newLine+=[chars]\n",
        "        lines[i] = newLine\n",
        "        if(newLine[0] not in dict_meters): #THIS IF STATEMENT ALLOWS FOR MULTIPLE PRONUNCIATIONS OF A WORD\n",
        "            dict_meters[newLine[0]]=[chars]\n",
        "        else:\n",
        "            if(chars not in dict_meters[newLine[0]]):\n",
        "                dict_meters[newLine[0]]+=[chars]\n",
        "    dict_meters[','] = ['']\n",
        "    dict_meters['.'] = ['']\n",
        "    \n",
        "# map pos to possible syllables\n",
        "pos_syllables = {}\n",
        "for k, v in pos_to_words.items():\n",
        "    pos_syllables[k] = set()\n",
        "    for w in v:\n",
        "        try:\n",
        "            pos_syllables[k].add(len(dict_meters[w][0]))\n",
        "        except:\n",
        "            continue\n",
        "pos_syllables[','].add(0)\n",
        "pos_syllables['.'].add(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMPttlMlpzq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def valid_permutation_sylls(num_sylls, template, last_word_sylls):\n",
        "        \"\"\"\n",
        "        Finds and returns the first integer partition of num_sylls with a total\n",
        "        number of integers equal to the length of template - 1 for which each\n",
        "        assignment of syllables to pos is valid.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_sylls : int\n",
        "            The total number of syllables to be distributed across the words in\n",
        "            the line.\n",
        "        template : list\n",
        "            A list of str containing the pos for each word in the line.\n",
        "        last_word_sylls : int\n",
        "            The number of syllables of the last word in the line.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list\n",
        "            A list of ints corresponding to a valid assignment of syllables to\n",
        "            each word in the line.\n",
        "        \"\"\"\n",
        "        def get_all_partition_size_n(n, partition_size):\n",
        "            \"\"\"\n",
        "            Returns all integer partitions of an int with a partition_size number\n",
        "            of ints.\n",
        "            \"\"\"\n",
        "            def get_all_partitions(n, I=1):\n",
        "                yield (n,)\n",
        "                for i in range(I, n//2 + 1):\n",
        "                    for p in get_all_partitions(n-i, i):\n",
        "                        yield (i,) + p\n",
        "            return [p for p in get_all_partitions(n) if len(p) == partition_size]\n",
        "        def valid_syll(sylls, template):\n",
        "            \"\"\"\n",
        "            Checks if a template and syllable mapping are compatible.\n",
        "            \"\"\"\n",
        "            for i in range(len(template) - 1):\n",
        "                # Add in zeros to account for punctuation\n",
        "                if template[i] == ',' or template[i] == '.':\n",
        "                    sylls.insert(i, 0)\n",
        "                if sylls[i] not in pos_syllables[template[i]]:\n",
        "                    return False\n",
        "            return True\n",
        "        syllables_left = num_sylls - last_word_sylls\n",
        "        # Punctuation takes up no syllables, so subtract to get number of partitions\n",
        "        num_zero_sylls = sum(1 if pos == '.' or pos == ',' else 0 for pos in template)\n",
        "        num_words_left = len(template) - num_zero_sylls - 1\n",
        "\n",
        "        for partition in get_all_partition_size_n(syllables_left, num_words_left):\n",
        "            # Goes through all permutations by index, not numbers,\n",
        "            # inefficient implementation\n",
        "            permutations = list(itertools.permutations(partition))\n",
        "            random.shuffle(permutations)\n",
        "            for perm in permutations:\n",
        "                perm = list(perm)\n",
        "                # Last word is fixed\n",
        "                perm.append(last_word_sylls)\n",
        "                if valid_syll(perm, template):\n",
        "                    return perm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSgV4iYOCxHw",
        "colab_type": "code",
        "outputId": "5a0d02f9-2358-49dc-dbbf-9f05fb9d282f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "for k, l in pos_to_words.items():\n",
        "  if len(l) <= 10:\n",
        "    print(k, len(l))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WDT 7\n",
            ", 1\n",
            "WP 6\n",
            "TO 1\n",
            "PRP$ 8\n",
            "EX 4\n",
            "UH 7\n",
            "PDT 8\n",
            ". 2\n",
            "WP$ 1\n",
            "RBS 2\n",
            "SYM 0\n",
            "SO 1\n",
            "WHO 1\n",
            "THAN 1\n",
            "AS 1\n",
            "WHEN 1\n",
            "IF 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LcxYDpfEdj-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_colls_to_line(i, sequence, template, template_sylls):\n",
        "  curr_word = sequence[i]\n",
        "  if curr_word == '[MASK]':\n",
        "    return\n",
        "  colls = rev_coll_dict[curr_word]\n",
        "  for j in reversed(range(max(0, i - 3), i)):\n",
        "    for coll_word, score in colls:\n",
        "      if sequence[j] == '[MASK]' and coll_word not in sequence and template[j] in words_to_pos[coll_word] and coll_word in dict_meters and template_sylls[j] == len(dict_meters[coll_word][0]):\n",
        "        sequence[j] = coll_word\n",
        "\n",
        "def fill_from_back(sequence, template, template_sylls):\n",
        "  for i in reversed(range(len(sequence))):\n",
        "      add_colls_to_line(i, sequence, template, template_sylls)\n",
        "      \n",
        "def feed_bert_templates(text, masks, template, template_syll, no_punct=True):\n",
        "  tokenized_text = raw_text_to_tokens(text, masks)\n",
        "  \n",
        "  tokens_tensor, segment_tensor = token_segment_tensors(tokenized_text)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segment_tensor)\n",
        "  \n",
        "  pred_str = text.split()\n",
        "  if no_punct:\n",
        "    punct_indices = [1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]\n",
        "    top_indices = torch.argsort(predictions, dim=2, descending=True)\n",
        "    for i in masks:\n",
        "      for j in range(len(top_indices[0, i + 1])):\n",
        "        potential_prediction = top_indices[0, i + 1, j].item()\n",
        "        if potential_prediction not in punct_indices:\n",
        "          predicted_token = tokenizer.convert_ids_to_tokens([potential_prediction])[0]\n",
        "          if not (predicted_token in words_to_pos and predicted_token in dict_meters):\n",
        "            continue\n",
        "          if template[i] in words_to_pos[predicted_token] and len(dict_meters[predicted_token]) == template_syll[i]:\n",
        "            pred_str.pop(i)\n",
        "            pred_str.insert(i, predicted_token)\n",
        "            break\n",
        "  else:\n",
        "    for i in masks:\n",
        "      # Because we add [CLS] to the beginning of the sentence\n",
        "      predicted_index = torch.argmax(predictions[0, i + 1]).item()\n",
        "      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "      pred_str.pop(i)\n",
        "      pred_str.insert(i, predicted_token)\n",
        "  return pred_str, tokenized_text\n",
        "\n",
        "def try_all_low_freq_pos(w1, template, template_sylls):\n",
        "  # iterate over all permutions of low pos words\n",
        "  return\n",
        "\n",
        "def gen_line(w1, line=2, template=None, num_sylls=10):\n",
        "  pos = words_to_pos[w1][0]\n",
        "  if template is None:\n",
        "    template, original = random.choice(second_line_[pos])\n",
        "  \n",
        "  last_word_sylls = len(dict_meters[w1][0])\n",
        "  template_sylls = valid_permutation_sylls(num_sylls, template, last_word_sylls)\n",
        "  \n",
        "  seq = ['[MASK]' for i in range(len(template_sylls) - 1)]\n",
        "  seq.append(w1)\n",
        "  fill_from_back(seq, template, template_sylls)\n",
        "  \n",
        "  masks = [i for i in range(len(seq)) if seq[i] == '[MASK]']\n",
        "  as_str = ' '.join(seq)\n",
        "  print(as_str)\n",
        "  pred_seq, masked_tokens = feed_bert_templates(as_str, masks, template, template_sylls)\n",
        "  score = score_sentence_bert(' '.join(pred_seq))\n",
        "  \n",
        "  return pred_seq, score, template"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OiKlPyjrlpno",
        "colab_type": "code",
        "outputId": "0b37c168-c345-4dab-f963-f88420c55d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "seq = ['[MASK]', '[MASK]', '[MASK]', '[MASK]', 'amount']\n",
        "template = ['JJ', 'VB', 'JJ', 'VB', 'NN']\n",
        "template_sylls = [1, 1, 1, 1, 2]\n",
        "\n",
        "add_colls_to_line(4, seq, template, template_sylls)\n",
        "print(seq)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[MASK]', '[MASK]', 'fair', '[MASK]', 'amount']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xFCohf4YplMZ",
        "colab_type": "code",
        "outputId": "908b6908-d0b7-47d1-f6b8-98d35a0bea45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "gen_line('amount')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MASK] [MASK] [MASK] [MASK] [MASK] amount\n",
            "['[CLS]', 'who', 'were', 'all', '[MASK]', 'as', 'amount']\n",
            "[-15.141115188598633, -13.862958908081055, -13.31915283203125, -13.900723457336426, -13.831851959228516, -14.094440460205078]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['who', 'were', 'all', '[MASK]', 'as', 'amount'],\n",
              " -12.02146325792585,\n",
              " ['WHO', 'VBD', 'DT', 'NN', 'IN', 'NN'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "_X8CJmjdxp8i",
        "colab_type": "code",
        "outputId": "8da51da0-220a-4d95-98a4-e475bcdedce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "punct = tokenizer.tokenize('\\\" . , / \\' - _ ) (')\n",
        "print(punct)\n",
        "tokenizer.convert_tokens_to_ids(punct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"', '.', ',', '/', \"'\", '-', '_', ')', '(']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1000, 1012, 1010, 1013, 1005, 1011, 1035, 1007, 1006]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "U0OzTAddAen5",
        "colab_type": "code",
        "outputId": "d5a2f95e-6f6c-4b64-c06b-bb6a03f547d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "poetic_lines = [\"There was an Old Man with a beard Who said It is just as I feared\",\n",
        "                \"Two Owls and a Hen Four Larks and a Wren\",\n",
        "                \"Have all built their nests in my beard\",\n",
        "               \"There was a Young Lady of Dorking Who bought a large bonnet for walking\",\n",
        "                \"But its colour and size So bedazzled her eyes\",\n",
        "                \"That she very soon went back to Dorking\"]\n",
        "\n",
        "generated_lines = [\"who at years deserted last night on sky\",\n",
        "                   \"told me for what no birth if i heard the blue earth\",\n",
        "                   \"the extraordinary way you lie\",\n",
        "                  \"of the whole thing everything alternate who fascinated me now as a mate\",\n",
        "                   \"i continued , with a dream undertone eyes and the team\",\n",
        "                   \"cant sing the fascination of her state\"]\n",
        "\n",
        "poetic_scores = []\n",
        "for line in poetic_lines:\n",
        "  poetic_scores.append(score_sentence_bert(line))\n",
        "  print(line)\n",
        "  \n",
        "generated_scores = []\n",
        "for line in generated_lines:\n",
        "  generated_scores.append(score_sentence_bert(line))\n",
        "  print(line)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'there', 'was', 'an', 'old', 'man', 'with', 'a', 'beard', 'who', 'said', 'it', 'is', 'just', 'as', 'i', 'feared']\n",
            "[-15.141115188598633, -12.286706924438477, -12.675008773803711, -13.918147087097168, -13.837422370910645, -14.138650894165039, -14.68960952758789, -14.954139709472656, -14.437015533447266, -14.79336166381836, -13.459794998168945, -13.66992473602295, -13.885150909423828, -14.508549690246582, -13.66031551361084, -14.599510192871094]\n",
            "There was an Old Man with a beard Who said It is just as I feared\n",
            "['[CLS]', 'two', 'owls', 'and', 'a', 'hen', 'four', 'lark', '##s', 'and', 'a', 'wren']\n",
            "[-15.141115188598633, -14.01447868347168, -14.642720222473145, -14.090720176696777, -13.978071212768555, -13.346451759338379, -14.218205451965332, -15.601720809936523, -13.77589225769043, -14.648971557617188, -13.974457740783691]\n",
            "Two Owls and a Hen Four Larks and a Wren\n",
            "['[CLS]', 'have', 'all', 'built', 'their', 'nests', 'in', 'my', 'beard']\n",
            "[-15.141115188598633, -13.587356567382812, -13.417247772216797, -13.32610034942627, -13.881568908691406, -13.862619400024414, -14.177062034606934, -15.390162467956543]\n",
            "Have all built their nests in my beard\n",
            "['[CLS]', 'there', 'was', 'a', 'young', 'lady', 'of', 'do', '##rkin', '##g', 'who', 'bought', 'a', 'large', 'bonnet', 'for', 'walking']\n",
            "[-15.141115188598633, -12.286706924438477, -12.675008773803711, -14.104532241821289, -13.652532577514648, -13.942829132080078, -14.335205078125, -13.55866527557373, -12.095454216003418, -13.417795181274414, -14.02922248840332, -14.933082580566406, -14.380844116210938, -14.335196495056152, -13.649181365966797, -14.783445358276367]\n",
            "There was a Young Lady of Dorking Who bought a large bonnet for walking\n",
            "['[CLS]', 'but', 'its', 'colour', 'and', 'size', 'so', 'bed', '##az', '##zle', '##d', 'her', 'eyes']\n",
            "[-15.141115188598633, -13.536195755004883, -13.30975341796875, -13.925684928894043, -12.908093452453613, -13.721887588500977, -13.41691780090332, -13.404041290283203, -12.412331581115723, -12.83442497253418, -12.383883476257324, -13.118358612060547]\n",
            "But its colour and size So bedazzled her eyes\n",
            "['[CLS]', 'that', 'she', 'very', 'soon', 'went', 'back', 'to', 'do', '##rkin', '##g']\n",
            "[-15.141115188598633, -12.760953903198242, -14.570377349853516, -14.832463264465332, -14.146479606628418, -13.865888595581055, -14.909444808959961, -14.26722526550293, -14.185141563415527, -12.927114486694336]\n",
            "That she very soon went back to Dorking\n",
            "['[CLS]', 'who', 'at', 'years', 'deserted', 'last', 'night', 'on', 'sky']\n",
            "[-15.141115188598633, -13.862958908081055, -14.177719116210938, -12.988092422485352, -13.235183715820312, -13.993660926818848, -13.88113021850586, -13.996922492980957]\n",
            "who at years deserted last night on sky\n",
            "['[CLS]', 'told', 'me', 'for', 'what', 'no', 'birth', 'if', 'i', 'heard', 'the', 'blue', 'earth']\n",
            "[-15.141115188598633, -13.194348335266113, -13.49009895324707, -14.423521041870117, -12.878868103027344, -13.188740730285645, -13.806846618652344, -14.990436553955078, -14.400728225708008, -13.458734512329102, -14.584299087524414, -13.46316146850586]\n",
            "told me for what no birth if i heard the blue earth\n",
            "['[CLS]', 'the', 'extraordinary', 'way', 'you', 'lie']\n",
            "[-15.141115188598633, -13.953408241271973, -13.016860961914062, -13.635793685913086, -14.18028736114502]\n",
            "the extraordinary way you lie\n",
            "['[CLS]', 'of', 'the', 'whole', 'thing', 'everything', 'alternate', 'who', 'fascinated', 'me', 'now', 'as', 'a', 'mate']\n",
            "[-15.141115188598633, -12.87812328338623, -13.669767379760742, -12.460668563842773, -13.29180908203125, -13.926616668701172, -14.199039459228516, -14.114892959594727, -13.662461280822754, -14.006479263305664, -14.28254508972168, -14.58117961883545, -13.719746589660645]\n",
            "of the whole thing everything alternate who fascinated me now as a mate\n",
            "['[CLS]', 'i', 'continued', ',', 'with', 'a', 'dream', 'under', '##tone', 'eyes', 'and', 'the', 'team']\n",
            "[-15.141115188598633, -12.397459030151367, -14.101753234863281, -14.503713607788086, -14.395990371704102, -14.170354843139648, -13.970682144165039, -14.352154731750488, -14.623027801513672, -13.906976699829102, -15.28161907196045, -15.50830078125]\n",
            "i continued , with a dream undertone eyes and the team\n",
            "['[CLS]', 'can', '##t', 'sing', 'the', 'fascination', 'of', 'her', 'state']\n",
            "[-15.141115188598633, -14.511663436889648, -13.510708808898926, -13.768266677856445, -14.28079605102539, -12.883865356445312, -14.11760139465332, -14.110452651977539]\n",
            "cant sing the fascination of her state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QibK8Mm5DZ62",
        "colab_type": "code",
        "outputId": "5a47474a-2da8-4317-98e6-6bc33ad8fa7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sum(generated_scores) / len(generated_scores)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12.576219562985424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "R_zl8tXxk7UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff2609af-c539-40b1-d31d-2b0594caeea7"
      },
      "cell_type": "code",
      "source": [
        "sum(poetic_scores) / len(poetic_scores)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12.84572671754764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "biymnRFNW_bj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GPT Experiments"
      ]
    },
    {
      "metadata": {
        "id": "yE0w7-x0xdD9",
        "colab_type": "code",
        "outputId": "2dd34b85-c695-4d0e-cb47-77b13efa3a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2737
        }
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 12407131.58B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 8036751.99B/s]\n",
            "100%|██████████| 548118077/548118077 [00:11<00:00, 46582573.99B/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 31333.99B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): GPT2LMHead(\n",
              "    (decoder): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "W7gN4_Bj6mv1",
        "colab_type": "code",
        "outputId": "55d4b84a-3d68-437f-d22b-f3b54fb7cae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def feed_gpt(text):\n",
        "  indexed_tokens = tokenizer.encode(text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  \n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  model.to('cuda')\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    predictions, past = model(tokens_tensor)\n",
        "    \n",
        "  predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "  predicted_token = tokenizer.decode([predicted_index])\n",
        "  \n",
        "  return predicted_token\n",
        "\n",
        "text = 'Who was Jim Henson? Why was he'\n",
        "feed_gpt(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' so'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "ARjQ2a7X7p1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}